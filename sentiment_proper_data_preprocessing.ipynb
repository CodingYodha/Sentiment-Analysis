{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_df = pd.read_csv(r\"E:\\Projects\\datasets\\IMDB Dataset.csv\\IMDB Dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = r_df[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>\"American Nightmare\" is officially tied, in my...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>First off, I have to say that I loved the book...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>This movie was extremely boring. I only laughe...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>I was disgusted by this movie. No it wasn't be...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>Such a joyous world has been created for us in...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                review sentiment\n",
       "0    One of the other reviewers has mentioned that ...  positive\n",
       "1    A wonderful little production. <br /><br />The...  positive\n",
       "2    I thought this was a wonderful way to spend ti...  positive\n",
       "3    Basically there's a family where a little boy ...  negative\n",
       "4    Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
       "..                                                 ...       ...\n",
       "495  \"American Nightmare\" is officially tied, in my...  negative\n",
       "496  First off, I have to say that I loved the book...  negative\n",
       "497  This movie was extremely boring. I only laughe...  negative\n",
       "498  I was disgusted by this movie. No it wasn't be...  negative\n",
       "499  Such a joyous world has been created for us in...  positive\n",
       "\n",
       "[500 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=r_df['sentiment']\n",
    "x=r_df['review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.tag import pos_tag\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from textblob import TextBlob\n",
    "import contractions\n",
    "import emoji\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextPreprocessor:\n",
    "    def __init__(self , remove_stopwords=True , use_lemmatization=True):\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.stemmer = PorterStemmer()\n",
    "        self.remove_stopwords = remove_stopwords\n",
    "        self.use_lemmatization = use_lemmatization\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        #add custom stopwords that might be irrelevant for sentiment\n",
    "        self.stop_words.update(['movie' , 'film' , 'show' , 'watch' , 'seen'])\n",
    "\n",
    "    def get_wordnet_pos(self, treebank_tag):\n",
    "        \"\"\"\n",
    "        Map POS tag to first character lemmatize() accepts.\n",
    "        \"\"\"\n",
    "        if treebank_tag.startswith('J'):\n",
    "            return wordnet.ADJ\n",
    "        elif treebank_tag.startswith('V'):\n",
    "            return wordnet.VERB\n",
    "        elif treebank_tag.startswith('N'):\n",
    "            return wordnet.NOUN\n",
    "        elif treebank_tag.startswith('R'):\n",
    "            return wordnet.ADV\n",
    "        else:\n",
    "            return wordnet.NOUN\n",
    "\n",
    "    def clean_text(self , text):\n",
    "        \"\"\"Initial text cleaning\"\"\"\n",
    "        #convert to lower case\n",
    "        text = text.lower()\n",
    "\n",
    "        #replace contractions\n",
    "        text = contractions.fix(text)\n",
    "\n",
    "        #convert emojis to text\n",
    "        text = emoji.demojize(text)\n",
    "\n",
    "        #remove urls\n",
    "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "\n",
    "        #remove special characters and numbers\n",
    "        text = re.sub(r'[^a-zA-Z\\s]' , ' ' , text)\n",
    "\n",
    "        #remove html tags\n",
    "        text = re.sub(r'<.*?>','',text)\n",
    "\n",
    "        #handle extra whitespace\n",
    "        text = ' '.join(text.split())\n",
    "\n",
    "        return text\n",
    "    \n",
    "    def handle_negation(self , tokens):\n",
    "        \"\"\"Add NOT_prefix to words following negative words until punctuation\"\"\"\n",
    "        negation_words = {'not' , 'no' , 'never' , 'none' , \"n't\" , 'neither' , 'nor'}\n",
    "        negated=[]\n",
    "        negate = False\n",
    "\n",
    "        for token in tokens:\n",
    "            if token in negation_words:\n",
    "                negate=True\n",
    "            elif token in string.punctuation:\n",
    "                negate = False\n",
    "            elif negate:\n",
    "                token = f'NOT_{token}'\n",
    "            negated.append(token)\n",
    "        \n",
    "        return negated\n",
    "    \n",
    "    def preprocess(self , text):\n",
    "        \"\"\"Complete preprocessing pipeline\"\"\"\n",
    "\n",
    "        #clean text\n",
    "        text = self.clean_text(text)\n",
    "\n",
    "        #tokenize\n",
    "        tokens = word_tokenize(text)\n",
    "\n",
    "        #handle negtaion\n",
    "        tokens = self.handle_negation(tokens)\n",
    "\n",
    "        #remove stopwords if enabled\n",
    "        if self.remove_stopwords:\n",
    "            tokens = [token for token in tokens if token.lower() not in self.stop_words]\n",
    "\n",
    "        #lemmatize or stem based on configuration\n",
    "        if self.use_lemmatization:\n",
    "            tokens = [self.lemmatizer.lemmatize(token , self.get_wordnet_pos(token))\n",
    "                      for token in tokens]\n",
    "            \n",
    "        else:\n",
    "            tokens = [self.stemmer.stem(token) for token in tokens]\n",
    "\n",
    "        return tokens\n",
    "    \n",
    "class FeatureExtractor:\n",
    "    def __init__(self , max_features=5000 , ngram_range=(1,3)):\n",
    "        self.vectorizer = TfidfVectorizer(\n",
    "            max_features=max_features,\n",
    "            ngram_range = ngram_range,\n",
    "            min_df = 2,\n",
    "            max_df=0.95\n",
    "        )\n",
    "\n",
    "        def extract_features(self , texts):\n",
    "            #join tokens back into strings for Tf-IDf\n",
    "            processed_texts = [' '.join(tokens) for tokens in texts]\n",
    "            return self.vectorizer.fit_transform(processed_texts)\n",
    "        \n",
    "\n",
    "    #usage\n",
    "def prepare_data(df):\n",
    "    #initialize preprocessor\n",
    "    preprocessor = TextPreprocessor(\n",
    "        remove_stopwords=True,\n",
    "        use_lemmatization=True\n",
    "    )\n",
    "\n",
    "    #preprocess texts\n",
    "    df['preprocessed_tokens'] = df['review'].apply(preprocessor.preprocess)\n",
    "\n",
    "    #add sentiment-related features\n",
    "    df['text_length'] = df['review'].apply(len)\n",
    "    df['word_count'] = df['review'].apply(lambda x: len(x.split()))\n",
    "    df['avg_word_length'] = df['text_length']/df['word_count']\n",
    "\n",
    "    #add sentiment polarity using textblob\n",
    "    df['polarity'] = df['review'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "    df['subjectivity'] = df['review'].apply(lambda x: TextBlob(x).sentiment.subjectivity)\n",
    "\n",
    "    #extract tf-idf features\n",
    "    feature_extractor = FeatureExtractor(max_features=5000 , ngram_range=(1,3))\n",
    "    tfidf_features = feature_extractor.extract_features(df['processed_tokens'])\n",
    "\n",
    "    return df , tfidf_features\n",
    "        \n",
    "#training pipeline\n",
    "def train_model(df):\n",
    "    #prepare data\n",
    "    df , tfidf_features = prepare_data(df)\n",
    "\n",
    "    #split data\n",
    "    x = tfidf_features\n",
    "    y = df['sentiment']\n",
    "\n",
    "  # Add additional features\n",
    "    additional_features = df[['text_length', 'word_count', 'avg_word_length', \n",
    "                            'polarity', 'subjectivity']].values\n",
    "    \n",
    "    x_combined = np.hstack((x.toarray() ,additional_features))\n",
    "\n",
    "    return x_combined , y\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shiva\\AppData\\Local\\Temp\\ipykernel_25736\\3111974178.py:118: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['preprocessed_tokens'] = df['review'].apply(preprocessor.preprocess)\n",
      "C:\\Users\\Shiva\\AppData\\Local\\Temp\\ipykernel_25736\\3111974178.py:121: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['text_length'] = df['review'].apply(len)\n",
      "C:\\Users\\Shiva\\AppData\\Local\\Temp\\ipykernel_25736\\3111974178.py:122: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['word_count'] = df['review'].apply(lambda x: len(x.split()))\n",
      "C:\\Users\\Shiva\\AppData\\Local\\Temp\\ipykernel_25736\\3111974178.py:123: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['avg_word_length'] = df['text_length']/df['word_count']\n",
      "C:\\Users\\Shiva\\AppData\\Local\\Temp\\ipykernel_25736\\3111974178.py:126: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['polarity'] = df['review'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
      "C:\\Users\\Shiva\\AppData\\Local\\Temp\\ipykernel_25736\\3111974178.py:127: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['subjectivity'] = df['review'].apply(lambda x: TextBlob(x).sentiment.subjectivity)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'FeatureExtractor' object has no attribute 'extract_features'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 10\u001b[0m\n\u001b[0;32m      4\u001b[0m preprocessor \u001b[38;5;241m=\u001b[39m TextPreprocessor(\n\u001b[0;32m      5\u001b[0m     remove_stopwords\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m      6\u001b[0m     use_lemmatization\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m      7\u001b[0m )\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m#prepare features and train model\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m x,y \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m#split data with stratification\u001b[39;00m\n\u001b[0;32m     13\u001b[0m x_train , x_test , y_train , y_test \u001b[38;5;241m=\u001b[39m train_test_split(\n\u001b[0;32m     14\u001b[0m     x , y , test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m , random_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m42\u001b[39m , stratify\u001b[38;5;241m=\u001b[39my\n\u001b[0;32m     15\u001b[0m )\n",
      "Cell \u001b[1;32mIn[20], line 138\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtrain_model\u001b[39m(df):\n\u001b[0;32m    137\u001b[0m     \u001b[38;5;66;03m#prepare data\u001b[39;00m\n\u001b[1;32m--> 138\u001b[0m     df , tfidf_features \u001b[38;5;241m=\u001b[39m \u001b[43mprepare_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;66;03m#split data\u001b[39;00m\n\u001b[0;32m    141\u001b[0m     x \u001b[38;5;241m=\u001b[39m tfidf_features\n",
      "Cell \u001b[1;32mIn[20], line 131\u001b[0m, in \u001b[0;36mprepare_data\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;66;03m#extract tf-idf features\u001b[39;00m\n\u001b[0;32m    130\u001b[0m feature_extractor \u001b[38;5;241m=\u001b[39m FeatureExtractor(max_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5000\u001b[39m , ngram_range\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m3\u001b[39m))\n\u001b[1;32m--> 131\u001b[0m tfidf_features \u001b[38;5;241m=\u001b[39m \u001b[43mfeature_extractor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_features\u001b[49m(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprocessed_tokens\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m    133\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m df , tfidf_features\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'FeatureExtractor' object has no attribute 'extract_features'"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "preprocessor = TextPreprocessor(\n",
    "    remove_stopwords=True,\n",
    "    use_lemmatization=True\n",
    ")\n",
    "\n",
    "#prepare features and train model\n",
    "x,y = train_model(df)\n",
    "\n",
    "#split data with stratification\n",
    "x_train , x_test , y_train , y_test = train_test_split(\n",
    "    x , y , test_size=0.2 , random_state = 42 , stratify=y\n",
    ")\n",
    "\n",
    "#Train models\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "import xgboost as xgb\n",
    "\n",
    "#create individual models\n",
    "nb = MultinomialNB()\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    max_depth=3,\n",
    "    objective='binary:logistic',\n",
    "    eval_metric='logloss',\n",
    "    colsample_bytree=0.7962, \n",
    "    gamma=0.2323,\n",
    "    learning_rate=0.0343,\n",
    "    max_depth=4,  # Changed from float to int\n",
    "    min_child_weight=1.585,\n",
    "    n_estimators=477,  # Changed from float to int\n",
    "    subsample=0.9828\n",
    ")\n",
    "\n",
    "#create ensembles\n",
    "ensemble = VotingClassifier(\n",
    "    estimators=[('nb',nb) , ('xgb' , xgb_model)],\n",
    "    voting='soft'\n",
    ")\n",
    "\n",
    "#train and evaluate\n",
    "ensemble.fit(x_train , y_train)\n",
    "y_pred = ensemble.predict(x_test)\n",
    "\n",
    "#print metrics\n",
    "print(classification_report(y_test , y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fixed bugs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Shiva\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.tag import pos_tag\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from textblob import TextBlob\n",
    "import contractions\n",
    "import emoji\n",
    "\n",
    "class TextPreprocessor:\n",
    "    def __init__(self, remove_stopwords=True, use_lemmatization=True):\n",
    "        # Download all required NLTK resources\n",
    "        resources = [\n",
    "            'punkt',\n",
    "            'stopwords',\n",
    "            'averaged_perceptron_tagger',  # Changed from averaged_perceptron_tagger_eng\n",
    "            'wordnet'\n",
    "        ]\n",
    "        \n",
    "        print(\"Downloading required NLTK resources...\")\n",
    "        for resource in resources:\n",
    "            try:\n",
    "                nltk.data.find(f'tokenizers/{resource}')\n",
    "            except LookupError:\n",
    "                print(f\"Downloading {resource}...\")\n",
    "                nltk.download(resource, quiet=True)\n",
    "            \n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.stemmer = PorterStemmer()\n",
    "        self.remove_stopwords = remove_stopwords\n",
    "        self.use_lemmatization = use_lemmatization\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        # Add custom stopwords that might be irrelevant for sentiment\n",
    "        self.stop_words.update(['movie', 'film', 'show', 'watch', 'seen'])\n",
    "        \n",
    "    def get_wordnet_pos(self, word):\n",
    "        \"\"\"Map POS tag to WordNet POS tag for better lemmatization\"\"\"\n",
    "        try:\n",
    "            tag = pos_tag([word])[0][1][0].upper()\n",
    "            tag_dict = {\n",
    "                \"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV\n",
    "            }\n",
    "            return tag_dict.get(tag, wordnet.NOUN)\n",
    "        except Exception as e:\n",
    "            # If POS tagging fails, return NOUN as default\n",
    "            return wordnet.NOUN\n",
    "    \n",
    "    def clean_text(self, text):\n",
    "        \"\"\"Initial text cleaning\"\"\"\n",
    "        if not isinstance(text, str):\n",
    "            text = str(text)\n",
    "            \n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Replace contractions\n",
    "        text = contractions.fix(text)\n",
    "        \n",
    "        # Convert emojis to text\n",
    "        text = emoji.demojize(text)\n",
    "        \n",
    "        # Remove URLs\n",
    "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "        \n",
    "        # Remove HTML tags\n",
    "        text = re.sub(r'<.*?>', '', text)\n",
    "        \n",
    "        # Handle special characters and numbers\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
    "        \n",
    "        # Remove extra whitespace\n",
    "        text = ' '.join(text.split())\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def handle_negation(self, tokens):\n",
    "        \"\"\"Add NOT_ prefix to words following negative words until punctuation\"\"\"\n",
    "        negation_words = {'not', 'no', 'never', 'none', \"n't\", 'neither', 'nor'}\n",
    "        negated = []\n",
    "        negate = False\n",
    "        \n",
    "        for token in tokens:\n",
    "            if token in negation_words:\n",
    "                negate = True\n",
    "            elif token in string.punctuation:\n",
    "                negate = False\n",
    "            elif negate:\n",
    "                token = f'NOT_{token}'\n",
    "            negated.append(token)\n",
    "        return negated\n",
    "    \n",
    "    def preprocess(self, text):\n",
    "        \"\"\"Complete preprocessing pipeline\"\"\"\n",
    "        try:\n",
    "            # Clean text\n",
    "            text = self.clean_text(text)\n",
    "            \n",
    "            # Tokenize\n",
    "            tokens = word_tokenize(text)\n",
    "            \n",
    "            # Handle negation\n",
    "            tokens = self.handle_negation(tokens)\n",
    "            \n",
    "            # Remove stopwords if enabled\n",
    "            if self.remove_stopwords:\n",
    "                tokens = [token for token in tokens if token.lower() not in self.stop_words]\n",
    "            \n",
    "            # Lemmatize or stem based on configuration\n",
    "            if self.use_lemmatization:\n",
    "                tokens = [self.lemmatizer.lemmatize(token, self.get_wordnet_pos(token)) \n",
    "                            for token in tokens]\n",
    "            else:\n",
    "                tokens = [self.stemmer.stem(token) for token in tokens]\n",
    "                \n",
    "            return tokens\n",
    "        except Exception as e:\n",
    "            print(f\"Error preprocessing text: {str(e)}\")\n",
    "            # Return original text split into tokens as fallback\n",
    "            return text.lower().split()\n",
    "\n",
    "class FeatureExtractor:\n",
    "    def __init__(self, max_features=5000, ngram_range=(1, 3)):\n",
    "        self.vectorizer = TfidfVectorizer(\n",
    "            max_features=max_features,\n",
    "            ngram_range=ngram_range,\n",
    "            min_df=2,\n",
    "            max_df=0.95\n",
    "        )\n",
    "        \n",
    "    def fit_transform(self, texts):\n",
    "        \"\"\"Fit and transform the texts to TF-IDF features\"\"\"\n",
    "        # Join tokens back into strings for TF-IDF\n",
    "        processed_texts = [' '.join(tokens) if isinstance(tokens, list) else tokens \n",
    "                         for tokens in texts]\n",
    "        return self.vectorizer.fit_transform(processed_texts)\n",
    "    \n",
    "    def transform(self, texts):\n",
    "        \"\"\"Transform new texts using fitted vectorizer\"\"\"\n",
    "        processed_texts = [' '.join(tokens) if isinstance(tokens, list) else tokens \n",
    "                         for tokens in texts]\n",
    "        return self.vectorizer.transform(processed_texts)\n",
    "\n",
    "def prepare_data(df):\n",
    "    \"\"\"Prepare data with all preprocessing steps and feature extraction\"\"\"\n",
    "    # Initialize preprocessor\n",
    "    preprocessor = TextPreprocessor(\n",
    "        remove_stopwords=True,\n",
    "        use_lemmatization=True\n",
    "    )\n",
    "    \n",
    "    # Preprocess texts\n",
    "    print(\"Preprocessing texts...\")\n",
    "    df['processed_tokens'] = df['review'].apply(preprocessor.preprocess)\n",
    "    \n",
    "    # Add sentiment-related features\n",
    "    print(\"Extracting basic features...\")\n",
    "    df['text_length'] = df['review'].apply(len)\n",
    "    df['word_count'] = df['review'].apply(lambda x: len(str(x).split()))\n",
    "    df['avg_word_length'] = df['text_length'] / df['word_count']\n",
    "    \n",
    "    # Add sentiment polarity using TextBlob\n",
    "    print(\"Calculating sentiment scores...\")\n",
    "    df['polarity'] = df['review'].apply(lambda x: TextBlob(str(x)).sentiment.polarity)\n",
    "    df['subjectivity'] = df['review'].apply(lambda x: TextBlob(str(x)).sentiment.subjectivity)\n",
    "    \n",
    "    # Extract TF-IDF features\n",
    "    print(\"Extracting TF-IDF features...\")\n",
    "    feature_extractor = FeatureExtractor(max_features=5000, ngram_range=(1, 3))\n",
    "    tfidf_features = feature_extractor.fit_transform(df['processed_tokens'])\n",
    "    \n",
    "    return df, tfidf_features\n",
    "# Load your data\n",
    "import nltk\n",
    "# Download the required NLTK resource\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "def train_model(df):\n",
    "    \"\"\"Prepare data and return features and target\"\"\"\n",
    "    # Prepare data\n",
    "    df, tfidf_features = prepare_data(df)\n",
    "    \n",
    "    # Split data\n",
    "    X = tfidf_features\n",
    "    y = df['sentiment']\n",
    "    \n",
    "    # Add additional features\n",
    "    additional_features = df[['text_length', 'word_count', 'avg_word_length', \n",
    "                            'polarity', 'subjectivity']].values\n",
    "    \n",
    "    # Combine TF-IDF with additional features\n",
    "    X_combined = np.hstack((X.toarray(), additional_features))\n",
    "    \n",
    "    return X_combined, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading required NLTK resources...\n",
      "Downloading stopwords...\n",
      "Downloading averaged_perceptron_tagger...\n",
      "Downloading wordnet...\n",
      "Preprocessing texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shiva\\AppData\\Local\\Temp\\ipykernel_16540\\709088271.py:160: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['processed_tokens'] = df['review'].apply(preprocessor.preprocess)\n",
      "C:\\Users\\Shiva\\AppData\\Local\\Temp\\ipykernel_16540\\709088271.py:164: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['text_length'] = df['review'].apply(len)\n",
      "C:\\Users\\Shiva\\AppData\\Local\\Temp\\ipykernel_16540\\709088271.py:165: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['word_count'] = df['review'].apply(lambda x: len(str(x).split()))\n",
      "C:\\Users\\Shiva\\AppData\\Local\\Temp\\ipykernel_16540\\709088271.py:166: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['avg_word_length'] = df['text_length'] / df['word_count']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting basic features...\n",
      "Calculating sentiment scores...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shiva\\AppData\\Local\\Temp\\ipykernel_16540\\709088271.py:170: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['polarity'] = df['review'].apply(lambda x: TextBlob(str(x)).sentiment.polarity)\n",
      "C:\\Users\\Shiva\\AppData\\Local\\Temp\\ipykernel_16540\\709088271.py:171: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['subjectivity'] = df['review'].apply(lambda x: TextBlob(str(x)).sentiment.subjectivity)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting TF-IDF features...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.87      0.84        53\n",
      "    positive       0.84      0.79      0.81        47\n",
      "\n",
      "    accuracy                           0.83       100\n",
      "   macro avg       0.83      0.83      0.83       100\n",
      "weighted avg       0.83      0.83      0.83       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Prepare features and train model\n",
    "X, y = train_model(df)\n",
    "\n",
    "# Scale features to range [0, 1]\n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Split data with stratification\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Train models\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "import xgboost as xgb\n",
    "\n",
    "# Create individual models\n",
    "nb = MultinomialNB()\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    max_depth=6,\n",
    "    n_estimators=200,\n",
    "    learning_rate=0.1\n",
    ")\n",
    "\n",
    "# Create ensemble\n",
    "ensemble = VotingClassifier(\n",
    "    estimators=[('nb', nb), ('xgb', xgb_model)],\n",
    "    voting='soft'\n",
    ")\n",
    "\n",
    "# Train and evaluate\n",
    "ensemble.fit(X_train, y_train)\n",
    "y_pred = ensemble.predict(X_test)\n",
    "\n",
    "# Print metrics\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu-2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
